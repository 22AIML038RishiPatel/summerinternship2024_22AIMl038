# Summer Internship 2024 - Python Developer at Mamo Technolabs

## ğŸ“… May 13th, 2024

### ğŸ“˜ Introduction
I completed my internship at Mamo Technolabs as a Python Developer. My internship focused on enhancing my Python programming skills through hands-on projects and real-world applications. At Mamo Technolabs, I collaborated on innovative software solutions, gaining experience in web development, data analysis, and automation. This opportunity allowed me to learn industry best practices and contribute to impactful projects.

### ğŸ’¼ Responsibilities
My responsibilities included:
- Developing and maintaining Python-based applications.
- Collaborating with the development team on various projects.
- Writing and optimizing code for performance.
- Debugging and troubleshooting software issues.

# ğŸ“Š Weekly Progress

### ğŸ—“ï¸ Week 1: May 13th - May 18th
- Extracted company profiles and LinkedIn profiles from Clutch.co using Python libraries such as BeautifulSoup and Scrapy.
- Ensured data accuracy and addressed errors during the extraction process.
- Adhered to best practices in ethical web scraping and data privacy.

#### Define the Scope
- Identified specific data points needed from Clutch.co, such as company names, profiles, LinkedIn URLs, reviews, ratings, services offered, and contact information.

#### Set Up the Environment
- Prepared the development environment with necessary Python libraries and tools, including BeautifulSoup, Scrapy, Requests, and Pandas for data manipulation.

### ğŸ—“ï¸ Week 2: May 20th - May 25th
#### ğŸ•¸ï¸ Web Scraping Implementation
- **Crawling:** Used Scrapy to navigate through the websiteâ€™s structure, ensuring efficient and comprehensive data collection.
- **Parsing:** Employed BeautifulSoup to parse the HTML content and extract relevant information.

#### Data Storage and Management
- Stored the scraped data in a structured format, such as CSV or a database (e.g., SQLite, MongoDB), ensuring easy access and analysis.
- Regularly updated and maintained the dataset to reflect the latest information from Clutch.co.
- Cross-checked the extracted data with source information to ensure accuracy.
- Used validation techniques to clean and standardize the data, removing duplicates and inconsistencies.

<img src="https://github.com/22AIML038RishiPatel/summerinternship2024_22AIMl038/assets/120238486/e0185a8a-c373-439c-a828-39d8a529294a" width="500" height="300" alt="Internship" />

### ğŸ—“ï¸ Week 3: May 27th - June 1st
- Executed a detailed Google search query to find email addresses of individuals with roles such as CEO, CFO, and HR in the cybersecurity field located in Boston.
- Targeted email addresses from domains like gmail.com, hotmail.com, and yahoo.com.

#### Using an Email Extraction Tool
- Utilized an email extraction tool or browser extension to automatically extract email addresses from search results, identifying a total of 46 email addresses.
- Streamlined the data collection process, reducing manual effort and increasing efficiency.
- Exported the extracted email addresses in various formats (TXT, CSV), facilitating easy integration into further data analysis or outreach activities.

<img src="https://github.com/22AIML038RishiPatel/summerinternship2024_22AIMl038/assets/120238486/59ab2b96-924b-4202-8135-f9e9906fe1a8" width="500" height="300" alt="Internship" />

### ğŸ—“ï¸ Week 4: June 3rd - June 8th
- Set up a connection to a PostgreSQL database using credentials stored in environment variables. Loaded these variables using the dotenv library to ensure secure handling of sensitive information.
- Created an engine for database interactions using the connection string.

#### Data Fetching and Updating Excel Sheet
- Implemented functions to fetch data from specified tables in the database and update an Excel sheet with this data using the xlwings library.
- The `fetch_data_from_database` function retrieved all rows and column names from the selected table, and `update_excel_sheet` wrote this data into the specified Excel sheet.

### ğŸ—“ï¸ Week 5: June 10th - June 15th
#### Uploading Data from Excel to Database
- Implemented a function, `main_u`, that reads data from an Excel sheet, processes it into a pandas DataFrame, and uploads it to the specified table in the database.
- Converted column names to lowercase and handled potential exceptions with appropriate error messages using the win32api library.
- Began working on a project for an AI ChatBot for the company, contributing individual tasks to the project.

### ğŸ—“ï¸ Week 6: June 17th - June 22nd
#### Objective
- Developed a chatbot using NLP to simulate human-like conversations and respond meaningfully to user queries.

#### Methodology
- **Preprocessing:** Tokenization and lemmatization of text data.
- **Vectorization:** Used TF-IDF to convert text into numerical vectors.
- **Response Generation:** Matched user inputs with appropriate responses using cosine similarity.

#### Implementation
- **Environment setup:** Installed necessary libraries and configured the development environment.
- **Processing the corpus:** Read and prepared text data.
- **Keyword matching:** Handled common greetings and interactions.
- **Continuous user interaction:** Ensured ongoing dialogue and appropriate response handling.

#### Testing and Feedback
- Conducted comprehensive testing and collected user feedback to improve performance and user experience.

### ğŸ› ï¸ Skills Learned
- Proficiency in using Python libraries like BeautifulSoup and Scrapy for web scraping and extracting data from websites.
- Experience in connecting to and managing databases, including tasks such as uploading and retrieving data using libraries like SQLAlchemy.
- Skills in automating data upload to Excel and loading data from Excel to a database using tools like xlwings.
- Ability to handle and process data efficiently, ensuring accuracy and usability, and automating repetitive tasks to improve workflow efficiency.
- These skills will be valuable for various roles in data science, software development, and database management.

### ğŸ“ Conclusion
Overall, this internship has been a valuable learning experience. I gained practical skills in Python development, visualization, and automation using Python. The projects I worked on have provided me with insights into real-world data challenges and solutions. I am grateful for the opportunity to contribute to the team and grow.
